---
title: 'Quickstart: Backend'
description: 'How to setup the CopilotCloud endpoints'
---

## Setup

### Setup the CopilotKit Backend Endpoint:
<Tabs>
  <Tab title="Manually specified endpoint">

      Currently all you require is an endpoint which serves as a proxy to an OpenAI-like LLM endpoint.
      See below for an example using NextJS edge functions.

      Make sure to install the `ai` and `openai` packages:
      <CodeGroup>
        ```bash npm
        npm i ai openai
        ``` 
        ```bash yarn
        yarn add ai openai
        ```
        ```bash pnpm
        pnpm add ai openai
        ```
      </CodeGroup>


      
      ```ts /api/copilotkit/chat/route.ts (NextJS)
      import { OpenAIStream, StreamingTextResponse } from "ai";
      import OpenAI from "openai";
      import { CompletionCreateParamsStreaming } from "openai/resources/chat/completions";

      const openai = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY,
      });

      export const runtime = "edge";

      export async function POST(req: Request): Promise<Response> {
        const forwardedProps = await req.json();

        const response = await openai.chat.completions.create({
          model: "gpt-4",
          ...forwardedProps,
          stream: true,
        } as CompletionCreateParamsStreaming);

        const stream = OpenAIStream(response, {
          experimental_onFunctionCall: async (
            { name, arguments: args },
            createFunctionCallMessages
          ) => {
            return undefined; // returning undefined to avoid sending any messages to the client when a function is called. Temporary, bc currently vercel ai sdk does not support returning both text and function calls -- although the API does support it.
          },
        });

        return new StreamingTextResponse(stream);
      }
      ```
  </Tab>
</Tabs>


